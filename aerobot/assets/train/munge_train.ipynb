{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/AF', '/KO', '/OGSE', '/WGE', '/aa_1mer', '/aa_2mer', '/aa_3mer', '/labels', '/nt_1mer', '/nt_2mer', '/nt_3mer', '/nt_4mer', '/nt_5mer']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file.h5' with the path to your HDF5 file\n",
    "h5_file_path = 'training_data.h5'\n",
    "\n",
    "# Read all keys (datasets) in the HDF5 file\n",
    "with pd.HDFStore(h5_file_path) as store:\n",
    "    keys = store.keys()\n",
    "\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/58pxvfhj0gb_wz2nzgrzc6pc0000gn/T/ipykernel_58368/1685733980.py:11: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block2_values] [items->Index(['checkm_marker_lineage', 'gtdb_genome_representative',\n",
      "       'gtdb_representative', 'gtdb_taxonomy', 'gtdb_type_designation',\n",
      "       'gtdb_type_designation_sources', 'gtdb_type_species_of_genus',\n",
      "       'lsu_23s_contig_len', 'lsu_23s_length', 'lsu_23s_query_id',\n",
      "       'lsu_5s_contig_len', 'lsu_5s_length', 'lsu_5s_query_id',\n",
      "       'lsu_silva_23s_blast_align_len', 'lsu_silva_23s_blast_bitscore',\n",
      "       'lsu_silva_23s_blast_evalue', 'lsu_silva_23s_blast_perc_identity',\n",
      "       'lsu_silva_23s_blast_subject_id', 'lsu_silva_23s_taxonomy',\n",
      "       'mimag_high_quality', 'mimag_low_quality', 'mimag_medium_quality',\n",
      "       'ncbi_assembly_level', 'ncbi_assembly_name', 'ncbi_assembly_type',\n",
      "       'ncbi_bioproject', 'ncbi_biosample', 'ncbi_contig_count',\n",
      "       'ncbi_contig_n50', 'ncbi_country', 'ncbi_date',\n",
      "       'ncbi_genbank_assembly_accession', 'ncbi_genome_category',\n",
      "       'ncbi_genome_representation', 'ncbi_isolate', 'ncbi_isolation_source',\n",
      "       'ncbi_lat_lon', 'ncbi_ncrna_count', 'ncbi_organism_name',\n",
      "       'ncbi_protein_count', 'ncbi_refseq_category', 'ncbi_rrna_count',\n",
      "       'ncbi_scaffold_count', 'ncbi_scaffold_l50', 'ncbi_scaffold_n50',\n",
      "       'ncbi_scaffold_n75', 'ncbi_scaffold_n90', 'ncbi_seq_rel_date',\n",
      "       'ncbi_ssu_count', 'ncbi_strain_identifiers', 'ncbi_submitter',\n",
      "       'ncbi_taxonomy', 'ncbi_taxonomy_unfiltered', 'ncbi_translation_table',\n",
      "       'ncbi_trna_count', 'ncbi_type_material_designation',\n",
      "       'ncbi_ungapped_length', 'ncbi_wgs_master', 'ssu_contig_len',\n",
      "       'ssu_gg_blast_align_len', 'ssu_gg_blast_bitscore',\n",
      "       'ssu_gg_blast_evalue', 'ssu_gg_blast_perc_identity',\n",
      "       'ssu_gg_blast_subject_id', 'ssu_gg_taxonomy', 'ssu_length',\n",
      "       'ssu_query_id', 'ssu_silva_blast_align_len', 'ssu_silva_blast_bitscore',\n",
      "       'ssu_silva_blast_evalue', 'ssu_silva_blast_perc_identity',\n",
      "       'ssu_silva_blast_subject_id', 'ssu_silva_taxonomy', 'domain',\n",
      "       'physiology', 'OXYGEN_REQUIREMENT', 'annotation_file',\n",
      "       'embedding_file'],\n",
      "      dtype='object')]\n",
      "\n",
      "  store[key] = dataframe\n"
     ]
    }
   ],
   "source": [
    "# Read each key (dataset) in the HDF5 file and store in the dictionary\n",
    "data_keys = {}\n",
    "with pd.HDFStore(h5_file_path) as store:\n",
    "    for key in store.keys():\n",
    "        dataframe = store[key]\n",
    "        genomes = dataframe.index.tolist()\n",
    "        # if \"_\" appears twice in the index, remove everything before the first \"_\" chacter\n",
    "        if len(genomes[0].split(\"_\")) > 2:\n",
    "            dataframe.index = [x.split(\"_\", 1)[1] for x in genomes]\n",
    "            # write dataframe back to the store\n",
    "            store[key] = dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each key (dataset) in the HDF5 file and store in the dictionary\n",
    "data_keys = {}\n",
    "with pd.HDFStore(h5_file_path) as store:\n",
    "    for key in store.keys():\n",
    "        dataframe = store[key]\n",
    "        genomes = dataframe.index.tolist()\n",
    "        data_keys[key] = genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/AF',\n",
       " '/KO',\n",
       " '/OGSE',\n",
       " '/WGE',\n",
       " '/aa_1mer',\n",
       " '/aa_2mer',\n",
       " '/aa_3mer',\n",
       " '/labels',\n",
       " '/nt_1mer',\n",
       " '/nt_2mer',\n",
       " '/nt_3mer',\n",
       " '/nt_4mer',\n",
       " '/nt_5mer']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with cds_1mer\n",
      "Done with cds_2mer\n",
      "Done with cds_3mer\n",
      "Done with cds_4mer\n",
      "Done with cds_5mer\n"
     ]
    }
   ],
   "source": [
    "kmers = [1,2,3,4,5]\n",
    "for k in kmers:\n",
    "    df = pd.read_csv(f\"aerobot_fna_proteins_{k}mer_features.csv\",index_col=0)\n",
    "    genomes = df.index.tolist()\n",
    "    if len(genomes[0].split(\"_\")) > 2:\n",
    "        df.index = [x.split(\"_\", 1)[1] for x in genomes]\n",
    "    key = f\"cds_{k}mer\"\n",
    "    # write dataframe to store\n",
    "    with pd.HDFStore(h5_file_path) as store:\n",
    "        store[key] = df\n",
    "        # let me know when done\n",
    "        print(f\"Done with {key}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each key (dataset) in the HDF5 file and store in the dictionary\n",
    "data_keys = {}\n",
    "with pd.HDFStore(h5_file_path) as store:\n",
    "    for key in store.keys():\n",
    "        dataframe = store[key]\n",
    "        genomes = dataframe.index.tolist()\n",
    "        data_keys[key] = genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/AF',\n",
       " '/KO',\n",
       " '/OGSE',\n",
       " '/WGE',\n",
       " '/aa_1mer',\n",
       " '/aa_2mer',\n",
       " '/aa_3mer',\n",
       " '/cds_1mer',\n",
       " '/cds_2mer',\n",
       " '/cds_3mer',\n",
       " '/cds_4mer',\n",
       " '/cds_5mer',\n",
       " '/labels',\n",
       " '/nt_1mer',\n",
       " '/nt_2mer',\n",
       " '/nt_3mer',\n",
       " '/nt_4mer',\n",
       " '/nt_5mer']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress the training_data.h5 to a .tar.gz file\n",
    "import tarfile\n",
    "with tarfile.open(\"training_data.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"training_data.h5\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aerobot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
